{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import json\n",
    "from time import time\n",
    "import pickle\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM\n",
    "from keras.layers.merge import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Text Captions\n",
    "\n",
    "def readTextFile(path):\n",
    "    with open(path) as f:\n",
    "        captions = f.read()\n",
    "    return captions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/shadabhussain/automated-image-captioning-flickr8/data\n",
    "\n",
    "captions  = readTextFile(\"./Data/Flickr_TextData/Flickr8k.token.txt\")\n",
    "captions = captions.split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first,second  = captions[0].split('\\t')\n",
    "print(first.split(\".\")[0])\n",
    "print(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to Map each Image with the list of captions it has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = {}\n",
    "\n",
    "for x in captions:\n",
    "    first,second = x.split('\\t')\n",
    "    img_name = first.split(\".\")[0]\n",
    "    \n",
    "    #if the image id is already present or not\n",
    "    if descriptions.get(img_name) is None:\n",
    "        descriptions[img_name] = []\n",
    "    \n",
    "    descriptions[img_name].append(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions[\"1000268201_693b08cb0e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = \"Data/Images/\"\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(IMG_PATH+\"1000268201_693b08cb0e.jpg\")\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(\"[^a-z]+\",\" \",sentence)\n",
    "    sentence = sentence.split()\n",
    "    \n",
    "    sentence  = [s for s in sentence if len(s)>1]\n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(\"A cat is sitting over the house # 64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all Captions\n",
    "for key,caption_list in descriptions.items():\n",
    "    for i in range(len(caption_list)):\n",
    "        caption_list[i] = clean_text(caption_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions[\"1000268201_693b08cb0e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data to text file\n",
    "with open(\"descriptions_1.txt\",\"w\") as f:\n",
    "    f.write(str(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = None\n",
    "with open(\"descriptions_1.txt\",'r') as f:\n",
    "    descriptions= f.read()\n",
    "    \n",
    "json_acceptable_string = descriptions.replace(\"'\",\"\\\"\")\n",
    "descriptions = json.loads(json_acceptable_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab\n",
    "\n",
    "vocab = set()\n",
    "for key in descriptions.keys():\n",
    "    [vocab.update(sentence.split()) for sentence in descriptions[key]]\n",
    "    \n",
    "print(\"Vocab Size : %d\"% len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total No of words across all the sentences\n",
    "total_words = []\n",
    "\n",
    "for key in descriptions.keys():\n",
    "    [total_words.append(i) for des in descriptions[key] for i in des.split()]\n",
    "    \n",
    "print(\"Total Words %d\"%len(total_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Words from the Vocab according to certain threshold frequncy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(total_words)\n",
    "freq_cnt = dict(counter)\n",
    "print(len(freq_cnt.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort this dictionary according to the freq count\n",
    "sorted_freq_cnt = sorted(freq_cnt.items(),reverse=True,key=lambda x:x[1])\n",
    "\n",
    "# Filter\n",
    "threshold = 10\n",
    "sorted_freq_cnt  = [x for x in sorted_freq_cnt if x[1]>threshold]\n",
    "total_words = [x[0] for x in sorted_freq_cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_data = readTextFile(\"Data/Flickr_TextData/Flickr_8k.trainImages.txt\")\n",
    "test_file_data = readTextFile(\"Data/Flickr_TextData/Flickr_8k.testImages.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [row.split(\".\")[0] for row in train_file_data.split(\"\\n\")[:-1]]\n",
    "test = [row.split(\".\")[0] for row in test_file_data.split(\"\\n\")[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Description for the Training Data\n",
    "# Tweak - Add <s> and <e> token to our training data\n",
    "train_descriptions = {}\n",
    "\n",
    "for img_id in train:\n",
    "    train_descriptions[img_id] = []\n",
    "    for cap in descriptions[img_id]:\n",
    "        cap_to_append = \"startseq \"  + cap + \" endseq\"\n",
    "        train_descriptions[img_id].append(cap_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions[\"1000268201_693b08cb0e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(weights=\"imagenet\",input_shape=(224,224,3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = Model(model.input,model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    img = image.load_img(img,target_size=(224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    # Normalisation\n",
    "    img = preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = preprocess_img(IMG_PATH+\"1000268201_693b08cb0e.jpg\")\n",
    "#plt.imshow(img[0])\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(img):\n",
    "    img = preprocess_img(img)\n",
    "    feature_vector = model_new.predict(img)\n",
    "    \n",
    "    feature_vector = feature_vector.reshape((-1,))\n",
    "    #print(feature_vector.shape)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_image(IMG_PATH+\"1000268201_693b08cb0e.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "encoding_train = {}\n",
    "#image_id -->feature_vector extracted from Resnet Image\n",
    "\n",
    "for ix,img_id in enumerate(train):\n",
    "    img_path = IMG_PATH+\"/\"+img_id+\".jpg\"\n",
    "    encoding_train[img_id] = encode_image(img_path)\n",
    "    \n",
    "    if ix%100==0:\n",
    "        print(\"Encoding in Progress Time step %d \"%ix)\n",
    "        \n",
    "end_t = time()\n",
    "print(\"Total Time Taken :\",end_t-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store everything to the disk \n",
    "with open(\"saved/encoded_train_features.pkl\",\"wb\") as f:\n",
    "    pickle.dump(encoding_train,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "encoding_test = {}\n",
    "#image_id -->feature_vector extracted from Resnet Image\n",
    "\n",
    "for ix,img_id in enumerate(test):\n",
    "    img_path = IMG_PATH+\"/\"+img_id+\".jpg\"\n",
    "    encoding_test[img_id] = encode_image(img_path)\n",
    "    \n",
    "    if ix%100==0:\n",
    "        print(\"Test Encoding in Progress Time step %d \"%ix)\n",
    "        \n",
    "end_t = time()\n",
    "print(\"Total Time Taken(test) :\",end_t-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"saved/encoded_test_features.pkl\",\"wb\") as f:\n",
    "    pickle.dump(encoding_test,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab\n",
    "len(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "\n",
    "for i,word in enumerate(total_words):\n",
    "    word_to_idx[word] = i+1\n",
    "    idx_to_word[i+1] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_to_idx[\"dog\"]\n",
    "#idx_to_word[1]\n",
    "print(len(idx_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two special words\n",
    "idx_to_word[1846] = 'startseq'\n",
    "word_to_idx['startseq'] = 1846\n",
    "\n",
    "idx_to_word[1847] = 'endseq'\n",
    "word_to_idx['endseq'] = 1847\n",
    "\n",
    "vocab_size = len(word_to_idx) + 1\n",
    "print(\"Vocab Size\",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0 \n",
    "for key in train_descriptions.keys():\n",
    "    for cap in train_descriptions[key]:\n",
    "        max_len = max(max_len,len(cap.split()))\n",
    "        \n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(train_descriptions,encoding_train,word_to_idx,max_len,batch_size):\n",
    "    X1,X2, y = [],[],[]\n",
    "    \n",
    "    n =0\n",
    "    while True:\n",
    "        for key,desc_list in train_descriptions.items():\n",
    "            n += 1\n",
    "            \n",
    "            photo = encoding_train[key+\".jpg\"]\n",
    "            for desc in desc_list:\n",
    "                \n",
    "                seq = [word_to_idx[word] for word in desc.split() if word in word_to_idx]\n",
    "                for i in range(1,len(seq)):\n",
    "                    xi = seq[0:i]\n",
    "                    yi = seq[i]\n",
    "                    \n",
    "                    #0 denote padding word\n",
    "                    xi = pad_sequences([xi],maxlen=max_len,value=0,padding='post')[0]\n",
    "                    yi = to_categorcial([yi],num_classes=vocab_size)[0]\n",
    "                    \n",
    "                    X1.append(photo)\n",
    "                    X2.append(xi)\n",
    "                    y.append(yi)\n",
    "                    \n",
    "                if n==batch_size:\n",
    "                    yield [[np.array(X1),np.array(X2)],np.array(y)]\n",
    "                    X1,X2,y = [],[],[]\n",
    "                    n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./saved/glove.6B.50d.txt\",encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    \n",
    "    word = values[0]\n",
    "    word_embedding = np.array(values[1:],dtype='float')\n",
    "    embedding_index[word] = word_embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix():\n",
    "    emb_dim = 50\n",
    "    matrix = np.zeros((vocab_size,emb_dim))\n",
    "    for word,idx in word_to_idx.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            matrix[idx] = embedding_vector\n",
    "            \n",
    "    return matrix\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_embedding_matrix()\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_matrix[1847]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img_features = Input(shape=(2048,))\n",
    "inp_img1 = Dropout(0.3)(input_img_features)\n",
    "inp_img2 = Dense(256,activation='relu')(inp_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Captions as Input\n",
    "input_captions = Input(shape=(max_len,))\n",
    "inp_cap1 = Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_captions)\n",
    "inp_cap2 = Dropout(0.3)(inp_cap1)\n",
    "inp_cap3 = LSTM(256)(inp_cap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder1 = add([inp_img2,inp_cap3])\n",
    "decoder2 = Dense(256,activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size,activation='softmax')(decoder2)\n",
    "\n",
    "# Combined Model\n",
    "model = Model(inputs=[input_img_features,input_captions],outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Thing - Embedding Layer\n",
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 3\n",
    "steps = len(train_descriptions)//number_pics_per_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        generator = data_generator(train_descriptions,encoding_train,word_to_idx,max_len,batch_size)\n",
    "        model.fit_generator(generator,epochs=1,steps_per_epoch=steps,verbose=1)\n",
    "        model.save('./model_weights/model_'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./model_weights/model_9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(photo):\n",
    "    \n",
    "    in_text = \"startseq\"\n",
    "    for i in range(max_len):\n",
    "        sequence = [word_to_idx[w] for w in in_text.split() if w in word_to_idx]\n",
    "        sequence = pad_sequences([sequence],maxlen=max_len,padding='post')\n",
    "        \n",
    "        ypred = model.predict([photo,sequence])\n",
    "        ypred = ypred.argmax() #WOrd with max prob always - Greedy Sampling\n",
    "        word = idx_to_word[ypred]\n",
    "        in_text += (' ' + word)\n",
    "        \n",
    "        if word == \"endseq\":\n",
    "            break\n",
    "    \n",
    "    final_caption = in_text.split()[1:-1]\n",
    "    final_caption = ' '.join(final_caption)\n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pick Some Random Images and See Results\n",
    "plt.style.use(\"seaborn\")\n",
    "for i in range(15):\n",
    "    idx = np.random.randint(0,1000)\n",
    "    all_img_names = list(encoding_test.keys())\n",
    "    img_name = all_img_names[idx]\n",
    "    photo_2048 = encoding_test[img_name].reshape((1,2048))\n",
    "    \n",
    "    i = plt.imread(\"Data/Images/\"+img_name+\".jpg\")\n",
    "    \n",
    "    caption = predict_caption(photo_2048)\n",
    "    #print(caption)\n",
    "    \n",
    "    plt.title(caption)\n",
    "    plt.imshow(i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8 (tags/v3.9.8:bb3fdcf, Nov  5 2021, 20:48:33) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b671c20432fcd147198c92e7f072af9e705f087eb990bee22b07f08caab9f630"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
